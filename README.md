# Dynamical Dual Latent Space Variational Autoencoder Model with Koopman Constraint for Semi-Supervised Soft Sensor Application
A PyTorch implementation of Dynamical Dual Latent Space Variational Autoencoder Model with Koopman Constraint for Semi-Supervised Soft Sensor Application.

## ğŸ“ Project Structure
```text
â”œâ”€â”€ data_pre/ # Data preprocessing modules
â”‚ â”œâ”€â”€ data_pre.py # Data preprocessing script
â”‚ â””â”€â”€ dataset.py # Dataset class definition
â”œâ”€â”€ dataset/ # Data directory
â”‚ â”œâ”€â”€ SRU_data.npy # Process data in numpy format
â”‚ â””â”€â”€ SRU_data.txt # Process data in text format
â”œâ”€â”€ figures/ # Generated figures and plots
â”œâ”€â”€ inference/ # Inference and evaluation
â”‚ â””â”€â”€ quality_estimate.py # Future quality estimation
â”œâ”€â”€ logs/ # Training logs and results
â”œâ”€â”€ model/ # Model architecture
â”‚ â””â”€â”€ D2KVAE.py # Dynamical Dual Latent Space Variational Autoencoder with Koopman Constraint
â”œâ”€â”€ models/ # Saved model checkpoints
â”œâ”€â”€ train/ # Training utilities
â”‚ â””â”€â”€ trainer.py # Training loop and loss functions
â”œâ”€â”€ utils/ # Utility functions
â”‚ â”œâ”€â”€ FC.py # Fully connected layers
â”‚ â””â”€â”€ scheduler.py # Learning rate schedulers
â”œâ”€â”€ main.py # Main execution script
â””â”€â”€ run.sh # Batch experiment runner
```

## ğŸš€ Features
- **Dual latent space architecture for independent representation**
- **Differentiated priors within a dynamic VAE framework**
- **Enhanced Koopman constraint with random steps**
- **Demonstrated effectiveness on industrial benchmarks**

## ğŸ› ï¸ Installation
```
pip install torch==2.2.2 numpy==1.26.4 matplotlib scikit-learn
```

## ğŸ“Š Usage

### Quick Start  
Run the batch experiments with different label rates:
```
# Make the script executable  
chmod +x run.sh  

# Execute all experiments  
./run.sh  
```

The experiment was conducted on:

- **GPU**: NVIDIA 3090  
- **CPU**: Intel(R) Xeon(R) Silver 4214 CPU @ 2.70 GHz  
- **OS**: Ubuntu 18.04.5 LTS  
- **Python**: 3.10.8  



## ğŸ“ˆ Results

The model achieves excellent performance across different label rates:

| Label Rate | Test RMSE | Test MAE | RÂ² Score |
|------------|-----------|----------|----------|
| 50%        | 2.320Â±0.092   | 1.750Â±0.072  | 82.468Â±0.311   |
| 40%        | 2.357Â±0.086   | 1.763Â±0.072  | 80.356Â±1.971   |
| 30%        | 2.407Â±0.037   | 1.769Â±0.035  | 78.227Â±1.289   |
| 20%        | 2.458Â±0.134   | 1.858Â±0.108  | 77.237Â±2.043   |

## ğŸ“‹ Output Logs
Training progress and results are saved in the `./logs/` directory with timestamped filenames:

20250916_190905_number_1_seed_0_label_rate_0.2.log
20250916_190905_number_2_seed_0_label_rate_0.3.log
20250916_190905_number_3_seed_0_label_rate_0.4.log
20250916_190905_number_4_seed_0_label_rate_0.5.log
20250916_190905_number_5_seed_1_label_rate_0.2.log
20250916_190905_number_6_seed_1_label_rate_0.3.log
20250916_190905_number_7_seed_1_label_rate_0.4.log
20250916_190905_number_8_seed_1_label_rate_0.5.log
20250916_190905_number_9_seed_2_label_rate_0.2.log
20250916_190905_number_10_seed_2_label_rate_0.3.log
20250916_190905_number_11_seed_2_label_rate_0.4.log
20250916_190905_number_12_seed_2_label_rate_0.5.log

Each log contains:

- Training and validation losses (total, reconstruction, KL divergences, label, Koopman)  
- Learning rate scheduling information  
- Final evaluation metrics (RMSE, MAE, RÂ²)  
- Training and testing times  


## ğŸ“ Citation
If you use this code in your research, please cite:

```
```
